@@ -34,7 +34,7 @@
             "temperature": 0.7,
             "top_p": 0.95,
             "top_k": 100,
-            "max_tokens": 768
+            "max_tokens": 2048
         },
         "record": 
         {
diff --git a/cost/ex_rate.txt b/cost/ex_rate.txt
index e69de29..a9411da 100644
--- a/cost/ex_rate.txt
+++ b/cost/ex_rate.txt
@@ -0,0 +1 @@
+1416.8
\ No newline at end of file
diff --git a/llm/llama4-maverick-instruct-basic.py b/llm/llama4-maverick-instruct-basic.py
index 1aeeb93..2990a71 100644
--- a/llm/llama4-maverick-instruct-basic.py
+++ b/llm/llama4-maverick-instruct-basic.py
@@ -33,7 +33,7 @@ def call(prompt: str, llm_param: dict) -> str:
 
     response = requests.post(
         "https://api.fireworks.ai/inference/v1/chat/completions",
-        headers=headers, json=payload, timeout=40
+        headers=headers, json=payload, timeout=60
     )
     response.raise_for_status()
     return response.json()["choices"][0]["message"]["content"].strip()
diff --git a/llm/llm_router.py b/llm/llm_router.py
deleted file mode 100644
index c7b7bbd..0000000
--- a/llm/llm_router.py
+++ /dev/null