@@ -82,24 +77,21 @@ def generate_commit_and_record(
         final_commit = "[ERROR] ìµœì¢… ì»¤ë°‹ ë©”ì‹œì§€ ì‹¤íŒ¨"
         log(f"âŒ ìµœì¢… ì»¤ë°‹ ë©”ì‹œì§€ ì‹¤íŒ¨: {e}")
 
-    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
-    # ğŸ”¹ ê¸°ë¡ìš© ë©”ì‹œì§€ ìƒì„± (doc_writing)
-    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
+    # ê¸°ë¡ìš© ë©”ì‹œì§€
     combined_diff = "\n\n".join(diff_chunks)
-
-    e = f"""
-ë‹¤ìŒì€ Git ë ˆí¬ì˜ ì „ì²´ ë§¥ë½ ìš”ì•½ê³¼ ì „ì²´ ë³€ê²½ëœ diff ë‚´ìš©ì…ë‹ˆë‹¤.
-ì´ í”„ë¡œì íŠ¸ì˜ ì˜ë„, ìˆ˜ì •ì˜ ë§¥ë½, ê¸°ìˆ ì  ê³ ë¯¼ì„ ë‹´ì•„ ê¸°ìˆ  ë¬¸ì„œìš©ìœ¼ë¡œ ê¸°ë¡í•´ì£¼ì„¸ìš”.
-
-ğŸ§± Repo context:
+    e = f"""[Repo context]
 {repo_context}
 
-ğŸ§© ì „ì²´ diff:
+[ì „ì²´ diff]
 {combined_diff}
 
 {record_prompt_txt}
 """
-    prompt_vars["e"] = e
+    prompt_vars["e"] = {
+        "text": e,
+        "model": llm_cfg_record["model"][0],
+        "purpose": "record"
+    }
 
     try:
         record_msg = call_llm(e, llm_cfg_record)
diff --git a/scripts/context.py b/scripts/context.py
index b327dc5..e7909fe 100644
--- a/scripts/context.py
+++ b/scripts/context.py
@@ -55,12 +55,11 @@ def build_context(log, timestamp: str, llm_file_cfg: dict, llm_repo_cfg: dict):
     for path in valid_files:
         try:
             code = path.read_text(encoding="utf-8").strip()
+            if not code:
+                continue
         except:
             continue
 
-        if not code:
-            continue
-
         prompt = f"""ë‹¹ì‹ ì€ Git Repo ì „ì²´ë¥¼ ë¶„ì„í•˜ì—¬, Commit ë©”ì‹œì§€ ìƒì„± ë° ê¸°ìˆ  ë¬¸ì„œ ì‘ì„±ì„ ë„ìš¸ ìš”ì•½ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
 ì´ ìš”ì•½ì€ ì½”ë“œì˜ ë²„ì „ ê¸°ë¡, ê¸°ìˆ  ë¬¸ì„œí™”, ê·¸ë¦¬ê³  ë¯¸ë˜ì˜ ì‘ì—…ìë‚˜ ë‚˜ ìì‹ ì„ ìœ„í•œ êµ¬ì¡°ì  ì„¤ëª…ì„ ë§Œë“œëŠ” ë° ì‚¬ìš©ë  ê²ƒì…ë‹ˆë‹¤.
 
@@ -80,7 +79,11 @@ def build_context(log, timestamp: str, llm_file_cfg: dict, llm_repo_cfg: dict):
 """
 
         var_name = f"a{a_index}"
-        prompt_vars[var_name] = prompt
+        prompt_vars[var_name] = {
+            "text": prompt,
+            "model": llm_file_cfg["model"][0],
+            "purpose": "summary"
+        }
         a_index += 1
 
         try: